{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "import matplotlib.pyplot as plt\n",
    "from cleantext import clean\n",
    "from io import StringIO\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# URL of the CSV file\n",
    "url = 'https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv'\n",
    "\n",
    "# Fetching the content from the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Checking if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Reading CSV data using pandas\n",
    "    csv_data = StringIO(response.text)\n",
    "    df = pd.read_csv(csv_data)\n",
    "\n",
    "    # Displaying the first few rows of the DataFrame\n",
    "    #print(df.head())\n",
    "else:\n",
    "    print(f\"Failed to fetch data. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    # remove multiple white spaces/tabs or newlines\n",
    "    mul_uni_whitespaces = re.compile(r'(\\s)\\s+')\n",
    "    text = mul_uni_whitespaces.sub(r'\\1',text)\n",
    "    # extract dates with form\n",
    "    # YY-MM-DD HH-MM:SS\n",
    "    date = re.compile(r\"\"\"\n",
    "        # YY\n",
    "        ([0-1][0-9]{3}|20[0-1][0-9]|202[0-3])\n",
    "        -\n",
    "        # MM 01-12, DD: 01-31\n",
    "        (1[0-2]|0[1-9])\n",
    "        -\n",
    "        ([1-3]0|[0-2][1-9]|31)\n",
    "        # whitespace\n",
    "        \\ {1}\n",
    "        # HH-MM:SS format\n",
    "        ([0-1][0-9]|2[0-4]):([0-5][0-9]|60):([0-5][0-9].\\d*)\n",
    "        \"\"\", re.X)\n",
    "    text = date.sub(r'<DATE>',text)\n",
    "    # extract date with alphabetic name, year\n",
    "    # example: Jan DD, YYYY\n",
    "    date_2 = re.compile(r\"\"\"\n",
    "        ([A-Z]|[a-z])([a-z]+\\ )([1-3]0|[0-2][1-9]|31)\n",
    "        ,\\ {1}\n",
    "        # YY 0000-2023\n",
    "        ([0-1][0-9]{3}|20[0-1][0-9]|202[0-3])\n",
    "        \"\"\", re.X)\n",
    "    text = date_2.sub(r'<DATE>',text)\n",
    "    \n",
    "    number = re.compile(r\"\"\"\n",
    "        (\\d+[,\\.]\\d+)|(\\d+)  \n",
    "        (?![\\w])             # next character has to be a non-word letter\n",
    "        \"\"\", re.X)\n",
    "    text = number.sub(r'<NUM>',text)\n",
    "    \n",
    "    email = re.compile(r\"\"\"\n",
    "        [\\w-]+@[\\.\\w-]+              # if we have @ we assume it's an email \n",
    "        \"\"\", re.X)\n",
    "    text = email.sub(r'<EMAIL>',text)\n",
    "    \n",
    "    # website with prefix http and/or www.\n",
    "    website_http = re.compile(r\"\"\"\n",
    "        (https?://www\\.|              # http(s)://www. or www.\n",
    "        https?://|www\\.)              # or http(s)://\n",
    "        ([^ \\t\\n\\r\\f\\v,]+)             # capture rest of website    \n",
    "        \"\"\", re.X)\n",
    "    text = website_http.sub(r'<URL>',text)\n",
    "    # website with prefix http and/or www.\n",
    "    website = re.compile(r\"\"\"        # no https or www\n",
    "        [\\w-]+\\.[\\.\\w-]+             # assume website if we have a dot .\n",
    "        (/[^ \\t\\n\\r\\f\\v,]*)?         # /<address> optional     \n",
    "        \"\"\", re.X)\n",
    "    text = website.sub(r'<URL>',text)\n",
    "    return text\n",
    "\n",
    "def tokenize_content(content):\n",
    "    news_sample = {\n",
    "    'content': [],\n",
    "    'url_count': [],\n",
    "    'date_count': [],\n",
    "    'number_count': [],\n",
    "    'pre_100_freq': [],\n",
    "    'post_100_freq': [],\n",
    "    }\n",
    "    df = pd.DataFrame(news_sample)\n",
    "    for i in range (len(content)):\n",
    "        cleaned_news = clean_text(content['content'][i])\n",
    "        tokens = nltk.word_tokenize(cleaned_news)\n",
    "        # tokenize all alphabetic words\n",
    "        tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "        # count vocab before removal of stop words\n",
    "        pre_vocab, pre_vocab_freq = np.unique(tokens, return_counts=True)\n",
    "\n",
    "        pre_vocab = pre_vocab[0:50]\n",
    "        pre_vocab_freq = pre_vocab_freq[0:50]\n",
    "        \n",
    "        # error check\n",
    "        if np.where(pre_vocab == 'URL')[0].size > 0:\n",
    "            index = np.where(pre_vocab == 'URL')[0][0]\n",
    "            url_count = pre_vocab_freq[index]\n",
    "        else:\n",
    "            url_count = 0\n",
    "        \n",
    "        if np.where(pre_vocab == 'DATE')[0].size > 0:\n",
    "            index = np.where(pre_vocab == 'DATE')[0][0]\n",
    "            date_count = pre_vocab_freq[index]\n",
    "        else:\n",
    "            date_count = 0\n",
    "        \n",
    "        if np.where(pre_vocab == 'NUM')[0].size > 0:\n",
    "            index = np.where(pre_vocab == 'NUM')[0][0]\n",
    "            number_count = pre_vocab_freq[index]\n",
    "        else:\n",
    "            number_count = 0\n",
    "        \n",
    "\n",
    "        pre_vocab_freq_sorted = np.argsort(pre_vocab_freq)[::-1]\n",
    "        pre_100_freq = pre_vocab[pre_vocab_freq_sorted[0:100]]\n",
    "        # list of stop words\n",
    "        stop_words = stopwords.words('english')\n",
    "        # remove stop words by filtering \n",
    "        tokens_no_stop = [token for token in tokens if not token in stop_words]\n",
    "\n",
    "        # stem words using Porterstemmer\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens_stem = [stemmer.stem(token) for token in tokens_no_stop]\n",
    "\n",
    "        # count vocab after removal of stop words\n",
    "        post_vocab, post_vocab_freq = np.unique(tokens_stem, return_counts=True)\n",
    "        post_vocab_freq_sorted = np.argsort(post_vocab_freq)[::-1]\n",
    "        post_100_freq = post_vocab[post_vocab_freq_sorted[0:100]]\n",
    "\n",
    "        df.loc[len(df.index)] = [content['content'][i],\n",
    "                                url_count, \n",
    "                                date_count,\n",
    "                                number_count,\n",
    "                                str(pre_100_freq),\n",
    "                                str(post_100_freq),\n",
    "                                ]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>url_count</th>\n",
       "      <th>date_count</th>\n",
       "      <th>number_count</th>\n",
       "      <th>pre_100_freq</th>\n",
       "      <th>post_100_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sometimes the power of Christmas will make you...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['a' 'and' 'at' 'congregation' 'about' 'act' '...</td>\n",
       "      <td>['sermon' 'congreg' 'act' 'ladi' 'waffl' 'hous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AWAKENING OF 12 STRANDS of DNA – “Reconnecting...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>['NUM' 'URL' 'as' 'a' 'awakening' 'dna' 'is' '...</td>\n",
       "      <td>['num' 'strand' 'url' 'dna' 'awaken' 'strang' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Never Hike Alone: A Friday the 13th Fan Film U...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>['by' 'and' 'a' 'NUM' 'as' 'alone' 'camp' 'URL...</td>\n",
       "      <td>['friday' 'num' 'film' 'fan' 'disanti' 'kyle' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When a rare shark was caught, scientists were ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>['a' 'and' 'NUM' 'but' 'can' 'because' 'creatu...</td>\n",
       "      <td>['shark' 'num' 'fish' 'found' 'viper' 'first' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donald Trump has the unnerving ability to abil...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>['a' 'NUM' 'and' 'ability' 'americans' 'abc' '...</td>\n",
       "      <td>['poll' 'num' 'trump' 'interview' 'presid' 'ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Prison for Rahm, God’s Work And Many Others\\n\\...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>['and' 'a' 'NUM' 'as' 'about' 'all' 'be' 'beca...</td>\n",
       "      <td>['num' 'kid' 'would' 'peopl' 'right' 'law' 'me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>4 Useful Items for Your Tiny Home\\n\\nHeadline:...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['a' 'are' 'and' 'as' 'but' 'can' 'also' 'beca...</td>\n",
       "      <td>['home' 'tini' 'space' 'use' 'without' 'refrig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Former CIA Director Michael Hayden said Thursd...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['a' 'all' 'be' 'bit' 'collectively' 'countrie...</td>\n",
       "      <td>['presid' 'hayden' 'bit' 'collect' 'trump' 'im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Antonio Sabato Jr. says Hollywood's liberal el...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>['and' 'a' 'NUM' 'but' 'because' 'been' 'addre...</td>\n",
       "      <td>['num' 'tv' 'think' 'newsmax' 'winfrey' 'said'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Former U.S. President Bill Clinton on Monday c...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>['in' 'of' 'a' 'myanmar' 'journalists' 'is' 'c...</td>\n",
       "      <td>['journalist' 'myanmar' 'reuter' 'releas' 'imm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               content  url_count  date_count  \\\n",
       "0    Sometimes the power of Christmas will make you...          0           0   \n",
       "1    AWAKENING OF 12 STRANDS of DNA – “Reconnecting...          5           1   \n",
       "2    Never Hike Alone: A Friday the 13th Fan Film U...          3           0   \n",
       "3    When a rare shark was caught, scientists were ...          0           0   \n",
       "4    Donald Trump has the unnerving ability to abil...          0           1   \n",
       "..                                                 ...        ...         ...   \n",
       "245  Prison for Rahm, God’s Work And Many Others\\n\\...          2           0   \n",
       "246  4 Useful Items for Your Tiny Home\\n\\nHeadline:...          0           0   \n",
       "247  Former CIA Director Michael Hayden said Thursd...          0           0   \n",
       "248  Antonio Sabato Jr. says Hollywood's liberal el...          0           0   \n",
       "249  Former U.S. President Bill Clinton on Monday c...          1           0   \n",
       "\n",
       "     number_count                                       pre_100_freq  \\\n",
       "0               1  ['a' 'and' 'at' 'congregation' 'about' 'act' '...   \n",
       "1               7  ['NUM' 'URL' 'as' 'a' 'awakening' 'dna' 'is' '...   \n",
       "2               7  ['by' 'and' 'a' 'NUM' 'as' 'alone' 'camp' 'URL...   \n",
       "3               6  ['a' 'and' 'NUM' 'but' 'can' 'because' 'creatu...   \n",
       "4               7  ['a' 'NUM' 'and' 'ability' 'americans' 'abc' '...   \n",
       "..            ...                                                ...   \n",
       "245            20  ['and' 'a' 'NUM' 'as' 'about' 'all' 'be' 'beca...   \n",
       "246             1  ['a' 'are' 'and' 'as' 'but' 'can' 'also' 'beca...   \n",
       "247             0  ['a' 'all' 'be' 'bit' 'collectively' 'countrie...   \n",
       "248             9  ['and' 'a' 'NUM' 'but' 'because' 'been' 'addre...   \n",
       "249             2  ['in' 'of' 'a' 'myanmar' 'journalists' 'is' 'c...   \n",
       "\n",
       "                                         post_100_freq  \n",
       "0    ['sermon' 'congreg' 'act' 'ladi' 'waffl' 'hous...  \n",
       "1    ['num' 'strand' 'url' 'dna' 'awaken' 'strang' ...  \n",
       "2    ['friday' 'num' 'film' 'fan' 'disanti' 'kyle' ...  \n",
       "3    ['shark' 'num' 'fish' 'found' 'viper' 'first' ...  \n",
       "4    ['poll' 'num' 'trump' 'interview' 'presid' 'ne...  \n",
       "..                                                 ...  \n",
       "245  ['num' 'kid' 'would' 'peopl' 'right' 'law' 'me...  \n",
       "246  ['home' 'tini' 'space' 'use' 'without' 'refrig...  \n",
       "247  ['presid' 'hayden' 'bit' 'collect' 'trump' 'im...  \n",
       "248  ['num' 'tv' 'think' 'newsmax' 'winfrey' 'said'...  \n",
       "249  ['journalist' 'myanmar' 'reuter' 'releas' 'imm...  \n",
       "\n",
       "[250 rows x 6 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_content(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
